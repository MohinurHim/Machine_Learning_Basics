Подготовка данных (Data Preparation)
это процесс обработки и преобразования данных перед их использованием в анализе или моделировании в рамках задачи в области науки о данных (Data Science) и машинного обучения (Machine Learning). Этот процесс играет критическую роль в успехе любого проекта по анализу данных, поскольку качество данных напрямую влияет на качество итоговых результатов.
Вот несколько основных шагов подготовки данных и их значение:

Очистка данных (Data Cleaning):

Удаление или заполнение пропущенных значений.
Обработка дубликатов.
Коррекция ошибок данных и аномалий.
Обработка выбросов.
Преобразование данных (Data Transformation):

Масштабирование признаков (например, стандартизация или нормализация).
Преобразование категориальных переменных в числовые (например, с использованием кодирования категорий).
Преобразование формата данных (например, даты и времени).
Создание новых признаков на основе существующих (Feature Engineering).
Выбор признаков (Feature Selection):

Отбор наиболее важных признаков для моделирования.
Исключение нерелевантных или избыточных признаков.
Обработка несбалансированных классов (Handling Imbalanced Classes):

Применение методов сэмплирования для балансировки классов (например, андерсэмплинг или оверсэмплинг).
Разделение данных (Data Splitting):

Разделение данных на обучающую, валидационную и тестовую выборки для оценки качества модели.
Нормализация данных (Data Normalization):

Приведение данных к стандартным или равномерным распределениям.
Удаление шума (Noise Reduction):

Удаление лишних данных, которые могут помешать модели.
Обработка временных рядов (Time Series Processing):

Выравнивание данных по времени.
Извлечение признаков из временных данных.
Цель подготовки данных - создать чистый, структурированный и подходящий для анализа набор данных, который позволит получить максимальную информацию из него и построить качественные модели машинного обучения или провести анализ данных.

Для проведения наших экспериментов мы будем использовать набор данных о ценах на жилье в России. https://drive.google.com/file/d/1rVVMjbsXLlOaWqkef0V-zPoINi1DaMeK/view?usp=sharing

Мы не планируем полностью очищать весь набор данных, однако рассмотрим основные методы и операции на его основе.

Прежде чем мы начнем процесс очистки данных, важно ознакомиться с исходным датасетом. Давайте рассмотрим сами данные:


[ ]
from google.colab import drive
drive.mount('/content/drive')
Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount("/content/drive", force_remount=True).

[ ]
# импорт пакетов
import pandas as pd
import numpy as np
import seaborn as sns

import matplotlib.pyplot as plt
import matplotlib.mlab as mlab
import matplotlib
plt.style.use('ggplot')
from matplotlib.pyplot import figure

%matplotlib inline
matplotlib.rcParams['figure.figsize'] = (12,8)

pd.options.mode.chained_assignment = None



# чтение данных
df = pd.read_csv('/content/drive/MyDrive/sberbank.csv')

# shape and data types of the data
print(df.shape)
print(df.dtypes)

# отбор числовых колонок
df_numeric = df.select_dtypes(include=[np.number])
numeric_cols = df_numeric.columns.values
print(numeric_cols)

# отбор нечисловых колонок
df_non_numeric = df.select_dtypes(exclude=[np.number])
non_numeric_cols = df_non_numeric.columns.values
print(non_numeric_cols)

Этот код показывает нам, что набор данных состоит из 7662 строки и 291 столбец. Мы видим, являются ли эти столбцы числовыми или категориальными признаками.

Теперь мы можем пройдемся по ключевым проблемам данных и исправим их.


1. Отсутствующие данные
Отсутствующие значения – одна из самых сложных, но и самых распространенных проблем.

1.1. Как обнаружить?
Рассмотрим три метода обнаружения отсутствующих данных в наборе.

1.1.1. Тепловая карта пропущенных значений
Когда признаков в наборе не очень много, визуализируйте пропущенные значения с помощью тепловой карты.


[ ]
cols = df.columns[:30] # первые 30 колонок
# определяем цвета
# зеленый - пропущенные данные, синий - не пропущенные
colours = ['#000099', '#1aff12']
sns.heatmap(df[cols].isnull(), cmap=sns.color_palette(colours))


1.1.2. Процентный список пропущенных данных
Если в наборе много признаков и визуализация занимает много времени, можно составить список долей отсутствующих записей для каждого признака.

for col in df.columns:
    pct_missing = np.mean(df[col].isnull())
    print('{} - {}%'.format(col, round(pct_missing*100)))


1.1.3. Гистограмма пропущенных данных
Еще одна хорошая техника визуализации для наборов с большим количеством признаков – построение гистограммы для числа отсутствующих значений в записи.

Три этапа построения гистограммы
Создание индикаторов пропущенных значений:

 for col in df.columns:
     missing = df[col].isnull()
     num_missing = np.sum(missing)

 if num_missing > 0:
     print('created missing indicator for: {}'.format(col))
     df['{}_ismissing'.format(col)] = missing
Этот блок кода проходится по каждому столбцу col в датафрейме df.
missing = df[col].isnull() создает булеву маску для пропущенных значений в столбце.
num_missing = np.sum(missing) вычисляет количество пропущенных значений в столбце.
Если num_missing > 0, то выводится сообщение о создании индикатора пропущенных значений для столбца col, и затем создается новый столбец в датафрейме с именем {}_ismissing, который содержит булевы значения пропущенных значений.
Подсчет общего числа пропущенных значений:

 ismissing_cols = [col for col in df.columns if 'ismissing' in col]
 df['num_missing'] = df[ismissing_cols].sum(axis=1)
ismissing_cols = [col for col in df.columns if 'ismissing' in col] создает список столбцов, содержащих индикаторы пропущенных значений.
df['num_missing'] = df[ismissing_cols].sum(axis=1) создает новый столбец num_missing, который содержит общее количество пропущенных значений для каждой строки путем суммирования индикаторов пропущенных значений по всем столбцам.
Визуализация количества пропущенных значений:

# Создание DataFrame для гистограммы
num_missing_counts = df['num_missing'].value_counts().reset_index()
num_missing_counts.columns = ['num_missing_values', 'count']
num_missing_counts = num_missing_counts.sort_values(by='num_missing_values')

# Вывод таблицы с пропущенными значениями
missing_table = df.isnull().sum().reset_index()
missing_table.columns = ['column', 'num_missing']
missing_table = missing_table[missing_table['num_missing'] > 0]
print("Таблица с количеством пропущенных значений в каждом столбце:")
print(missing_table)

# Построение гистограммы
plt.figure(figsize=(10, 6))
plt.bar(num_missing_counts['num_missing_values'], num_missing_counts['count'], color='skyblue')
plt.xlabel('Number of Missing Values per Row')
plt.ylabel('Number of Rows')
plt.title('Histogram of Missing Values per Row')
plt.xticks(num_missing_counts['num_missing_values'])  # Установить метки по оси X для всех значений
plt.show()


# Создание индикаторов пропущенных значений
for col in df.columns:
    missing = df[col].isnull()
    num_missing = np.sum(missing)

    if num_missing > 0:
        print('created missing indicator for: {}'.format(col))
        df['{}_ismissing'.format(col)] = missing

# Построение гистограммы количества пропущенных значений
ismissing_cols = [col for col in df.columns if 'ismissing' in col]
df['num_missing'] = df[ismissing_cols].sum(axis=1)

# Создание DataFrame для гистограммы
num_missing_counts = df['num_missing'].value_counts().reset_index()
num_missing_counts.columns = ['num_missing_values', 'count']
num_missing_counts = num_missing_counts.sort_values(by='num_missing_values')

# Вывод таблицы с пропущенными значениями
missing_table = df.isnull().sum().reset_index()
missing_table.columns = ['column', 'num_missing']
missing_table = missing_table[missing_table['num_missing'] > 0]
print("Таблица с количеством пропущенных значений в каждом столбце:")
print(missing_table)

# Построение гистограммы
plt.figure(figsize=(10, 6))
plt.bar(num_missing_counts['num_missing_values'], num_missing_counts['count'], color='skyblue')
plt.xlabel('Number of Missing Values per Row')
plt.ylabel('Number of Rows')
plt.title('Histogram of Missing Values per Row')
plt.xticks(num_missing_counts['num_missing_values'])  # Установить метки по оси X для всех значений
plt.show()


1.2. Что делать с пропущенными значениями?
Существуют четыре самых распространенных техники.

1.2.1. Отбрасывание записей
Первая техника в статистике называется методом удаления по списку и заключается в простом отбрасывании записи, содержащей пропущенные значения. Это решение подходит только в том случае, если недостающие данные не являются информативными.

Для отбрасывания можно использовать и другие критерии. Например, из гистограммы, построенной в предыдущем разделе, мы узнали, что лишь небольшое количество строк содержат более 35 пропусков. Мы можем создать новый набор данных df_less_missing_rows, в котором отбросим эти строки.

ind_missing = df[df['num_missing'] > 35].index
df_less_missing_rows = df.drop(ind_missing, axis=0)
image.png

1.2.2. Отбрасывание признаков
NB! Отбрасывание признаков может применяться только для неинформативных признаков.

В процентном списке, построенном ранее, мы увидели, что признак hospital_beds_raion имеет высокий процент недостающих значений. Мы можем полностью отказаться от этого признака:

cols_to_drop = ['hospital_beds_raion']
df_less_hos_beds_raion = df.drop(cols_to_drop, axis=1)
image.png

1.2.3. Внесение недостающих значений
Для численных признаков можно воспользоваться методом статистического или вероятностного заполнения пропусков. Например, на место пропуска можно записать среднее или медианное значение, полученное из остальных записей.

Для категориальных признаков можно использовать в качестве заполнителя наиболее часто встречающееся значение.

Возьмем для примера признак life_sq и заменим все недостающие значения медианой этого признака:


med = df['life_sq'].median()
print(med)
df['life_sq'] = df['life_sq'].fillna(med)
30.4


df['life_sq']

А теперь примените ту же стратегию заполнения сразу для всех числовых признаков:

df_numeric = df.select_dtypes(include=[np.number])
numeric_cols = df_numeric.columns.values

for col in numeric_cols:
    missing = df[col].isnull()
    num_missing = np.sum(missing)

    if num_missing > 0:  # only do the imputation for the columns that have missing values.
        print('imputing missing values for: {}'.format(col))
        df['{}_ismissing'.format(col)] = missing
        med = df[col].median()
        df[col] = df[col].fillna(med)
#
df_non_numeric = df.select_dtypes(exclude=[np.number])
non_numeric_cols = df_non_numeric.columns.values

for col in non_numeric_cols:
    missing = df[col].isnull()
    num_missing = np.sum(missing)

    if num_missing > 0:  # only do the imputation for the columns that have missing values.
        print('imputing missing values for: {}'.format(col))
        df['{}_ismissing'.format(col)] = missing

        top = df[col].describe()['top'] # impute with the most frequent value.
        df[col] = df[col].fillna(top)

1.2.4. Замена недостающих значений
Еще один популярный метод - использование "дефолтого" плейсхолдера для пропусков, например, создать новую категорию MISSING для категориальных признаков или число -999 для числовых.

Таким образом, мы сохраняем данные о пропущенных значениях, что тоже может быть ценной информацией.

# категориальные признаки
df['sub_area'] = df['sub_area'].fillna('_MISSING_')

# численные признаки
df['life_sq'] = df['life_sq'].fillna(-999)
2. Нетипичные данные (выбросы)
Выбросы – это данные, которые существенно отличаются от других наблюдений. Они могут соответствовать реальным отклонениям, но могут быть и просто ошибками.

2.1. Как обнаружить выбросы?
Для численных и категориальных признаков используются разные методы изучения распределения, позволяющие обнаружить выбросы.

2.1.1. Гистограмма/коробчатая диаграмма
Если признак численный, можно построить гистограмму или коробчатую диаграмму (ящик с усами). Посмотрим на примере уже знакомого нам признака life_sq.

df['life_sq'].hist(bins=100)

Чтобы изучить особенность поближе, построим еще одну диаграмму.

df.boxplot(column=['life_sq'])

Видим, что есть несколько выбросов...

2.1.2. Описательная статистика
Отклонения численных признаков могут быть слишком четкими, чтобы не визуализироваться коробчатой диаграммой. Вместо этого можно проанализировать их описательную статистику.

Например, используйте describe() для признака life_sq. Что вы видите?

df['life_sq'].describe()

2.1.3. Столбчатая диаграмма
Для категориальных признаков можно построить столбчатую диаграмму – для визуализации данных о категориях и их распределении.

Например, распределение признака ecology вполне равномерно и допустимо.

df['ecology'].value_counts().plot.bar()
2.1.4. Другие методы
Для обнаружения выбросов можно использовать другие методы, например, построение точечной диаграммы, z-оценку или кластеризацию. На этом занятии они не рассматриваются.

2.2. Что делать?
Выбросы довольно просто обнаружить, но выбор способа их устранения слишком существенно зависит от специфики набора данных и целей проекта. Их обработка во многом похожа на обработку пропущенных данных, которую мы разбирали в предыдущем разделе. Можно удалить записи или признаки с выбросами, либо скорректировать их, либо оставить без изменений, но учесть при интерпретации результатов.

Многие задачи DS & ML требуют от нас избавлятся от информации, если если она не добавляет никакой ценности.

Три основных типа «ненужных» данных:

неинформативные признаки с большим количеством одинаковых значений,

нерелевантные признаки,

дубликаты записей.

Рассмотрим работу с каждым типом «ненужных» данных отдельно.

3. Неинформативные признаки
Если признак имеет слишком много строк с одинаковыми значениями, он не несет полезной информации для проекта.

3.1. Как обнаружить?
Давайте составим список признаков, у которых более 95% строк содержат одно и то же значение.

num_rows = len(df.index)
low_information_cols = []

for col in df.columns:
    cnts = df[col].value_counts(dropna=False)
    top_pct = (cnts/num_rows).iloc[0]

    if top_pct > 0.95:
        low_information_cols.append(col)
        print('{0}: {1:.5f}%'.format(col, top_pct*100))
        print(cnts)
        print()

3.2. Что делать?
Если после анализа причин получения повторяющихся значений вы пришли к выводу, что признак не несет полезной информации, используйте drop().

4. Нерелевантные признаки
Нерелевантные признаки обнаруживаются ручным отбором и оценкой значимости.

Например, признак, регистрирующий температуру на Марсе точно не имеет никакого отношения к прогнозированию цен на российское жилье. Если признак не имеет значения для проекта, его нужно исключить.

В нашем датасете есть такие признаки?

5. Дубликаты записей
Если значения признаков (всех или большинства) в двух разных записях совпадают, эти записи называются дубликатами.

5.1. Как обнаружить повторяющиеся записи?
Способ обнаружения дубликатов зависит от того, что именно мы считаем дубликатами.

Например, в случае когда в наборе данных есть уникальный идентификатор id.

Если две записи имеют одинаковый id, мы считаем, что это одна и та же запись.

Другой распространенный способ вычисления дубликатов: по набору ключевых признаков.

Например, неуникальными можно считать записи с одной и той же площадью жилья и годом постройки.

Найдем в нашем наборе дубликаты по группе критических признаков – full_sq,life_sq, floor, build_year, num_room.

5.2. Что делать с дубликатами?
Очевидно, что повторяющиеся записи нам не нужны, значит, их нужно исключить из набора.

Большая проблема очистки данных – разные форматы записей. Для корректной работы модели важно, чтобы набор данных соответствовал определенным стандартам – необходимо тщательное исследование с учетом специфики самих данных. Мы рассмотрим четыре самых распространенных несогласованности:

Разные регистры символов.
Разные форматы данных (например, даты).
Опечатки в значениях категориальных признаков.
Адреса.
6. Разные регистры символов
Непоследовательное использование разных регистров в категориальных значениях является очень распространенной ошибкой, которая может существенно повлиять на анализ данных.

6.1. Как обнаружить?
Давайте посмотрим на признак sub_area:


df['sub_area'].value_counts(dropna=False)
Если в какой-то записи вместо Poselenie Sosenskoe окажется poselenie sosenskoe, они будут расценены как два разных значения. Нас это не устраивает.

6.2. Что делать?
Эта проблема легко решается принудительным изменением регистра:

7. Разные форматы данных
Ряд данных в наборе находится не в том формате, с которым нам было бы удобно работать. Например, даты, записанные в виде строки, следует преобразовать в формат DateTime.

7.1. Как обнаружить?
Признак timestamp представляет собой строку, хотя является датой:


print (df['timestamp'])

7.2. Что же делать?
Чтобы было проще анализировать транзакции по годам и месяцам, значения признака timestamp следует преобразовать в удобный формат:

df['timestamp_dt'] = pd.to_datetime(df['timestamp'], format='%Y-%m-%d')
df['year'] = df['timestamp_dt'].dt.year
df['month'] = df['timestamp_dt'].dt.month
df['weekday'] = df['timestamp_dt'].dt.weekday

print(df['year'].value_counts(dropna=False))
print()
print(df['month'].value_counts(dropna=False))


8. Опечатки
Опечатки в значениях категориальных признаков приводят к таким же проблемам, как и разные регистры символов.

8.1. Как обнаружить?
В нашем наборе данных о недвижимости опечаток нет, поэтому для демонстрации принципов работы с опечатками создадим новый набор.

В нем будет признак city, а его значениями будут torontoo и tronto. В обоих случаях это опечатки, а правильное значение – toronto.

Простой способ идентификации опечаток – нечеткая логика или редактирование на основе расстояния. Суть этого метода заключается в измерении количества букв (расстояния), которые нам нужно изменить, чтобы из одного слова получить другое.

Предположим, нам известно, что в признаке city должно находиться одно из четырех значений: toronto, vancouver, montreal или calgary. Мы вычисляем расстояние между всеми значениями и словом toronto (и vancouver).

Те слова, в которых содержатся опечатки, имеют меньшее расстояние с правильным словом, так как отличаются всего на пару букв.

from nltk.metrics import edit_distance

df_city_ex = pd.DataFrame(data={'city': ['torontoo', 'toronto', 'tronto', 'vancouver', 'vancover', 'vancouvr', 'montreal', 'calgary']})


df_city_ex['city_distance_toronto'] = df_city_ex['city'].map(lambda x: edit_distance(x, 'toronto'))
df_city_ex['city_distance_vancouver'] = df_city_ex['city'].map(lambda x: edit_distance(x, 'vancouver'))
df_city_ex

8.2. Что делать?
Мы можем установить критерии для преобразования этих опечаток в правильные значения.

Например, если расстояние некоторого значения от слова toronto не превышает 2 буквы, мы преобразуем это значение в правильное – toronto.

9. Адреса
Адреса – реальная проблемма для всех аналитиков данных. Ведь мало кто следует стандартному формату, вводя свой адрес в базу данных.

9.1. Как обнаружить?
Проще предположить, что проблема разных форматов адреса точно существует. Даже если визуально вы не обнаружили ошибок в этом признаке, все равно стоит стандартизировать их для надежности.

В нашем наборе данных по соображениям конфиденциальности отсутствует признак адреса, поэтому создадим новый набор df_add_ex:

df_add_ex = pd.DataFrame(['123 MAIN St Apartment 15', '123 Main Street Apt 12   ', '543 FirSt Av', '  876 FIRst Ave.'], columns=['address'])
df_add_ex

9.2. Что мы можем сделать делать?
Форматирование данных обычно включает следующие операции:

приведение всех символов к нижнему регистру;

удаление пробелов в начале и конце строки;

удаление точек;

стандартизация формулировок: в случае с адресами это - замена street на st, apartment на apt и т. д.

Задание

Стандартизируйте адрес, выполнив следующие операции:

Создайте новый столбец 'address_std' в датафрейме df_add_ex, который будет содержать стандартизированные версии адресов. Строковые значения в столбце 'address' приведите к нижнему регистру с помощью метода str.lower().
Ваш код должен выглядеть следующим образом:

df_add_ex['address_std'] = df_add_ex['address'].str.lower()

Примените метод str.strip(), чтобы удалить лишние пробелы в начале и конце каждой строки в столбце 'address_std'.

Примените метод str.replace('\\.', ''), чтобы заменить все точки в строках столбца 'address_std' на пустые строки, то есть удалите точки из адресов.

Примените метод str.replace('\\bstreet\\b', 'st') для замены всех слов "street" (с учетом границ слова) в адресах на сокращенное обозначение "st".

Примените метод str.replace('\\bapartment\\b', 'apt') для замены слова "apartment" (с учетом границ слова) на сокращенную версию "apt".

Примените метод str.replace('\\bav\\b', 'ave') заменить слово "av" (с учетом границ слова) на "ave", что является стандартнымм сокращением "avenue".

Выведите изменный датафрейм df_add_ex

df_add_ex['address_std'] = df_add_ex['address'].str.lower()
df_add_ex['address_std'] = df_add_ex['address_std'].str.strip()
df_add_ex['address_std'] = df_add_ex['address_std'].str.replace('\\.', '')
df_add_ex['address_std'] = df_add_ex['address_std'].str.replace('\\bstreet\\b', 'st')
df_add_ex['address_std'] = df_add_ex['address_std'].str.replace('\\bapartment\\b', 'apt')
df_add_ex['address_std'] = df_add_ex['address_std'].str.replace('\\bav\\b', 'ave')

df_add_ex

Несколько важных моментов, которые стоило бы упомянуть:
Если вы будете производить заполнение пропусков на всём датасете сразу, это может привести к утечке тестовых данных в тренировочные.

z оценка и квартили могут использоваться в случае нормально распределенных признаков, в других случаях данным методам доверять несколько рискованно.
